{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tweets = pd.read_csv('data/harvey_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Cleaning Function\n",
    "### Cleans all the noise in the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return lemmatizer.lemmatize(lemmatizer.lemmatize(x, pos='v'))\n",
    "\n",
    "def tweet_clean(text):\n",
    "    '''function to clean each tweet\n",
    "    - Remove www nad http patterns\n",
    "    - Remove numbers and symbols\n",
    "    - Negation handling\n",
    "    - Lowercase\n",
    "    - Word stemming\n",
    "    - Remove words with less than 2 characters\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tok = WordPunctTokenizer()\n",
    "    \n",
    "#    remove invalid symbol (\"\\ufffd\")\n",
    "    try:\n",
    "        bom_removed = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"\")\n",
    "    except:\n",
    "        bom_removed = text\n",
    "    \n",
    "#    remove mentioned username and links(https and www) patterns\n",
    "    user_pat = r'@[A-Za-z0-9_]+'\n",
    "    http_pat1 = r'http://[^ ]+'\n",
    "    http_pat2 = r'https://[^ ]+'\n",
    "    www_pat = r'www.[^ ]+'\n",
    "    combined_pat = r'|'.join((user_pat, http_pat1, http_pat2, www_pat)) \n",
    "    pat_removed = re.sub(combined_pat, '')\n",
    "    lower_text = pat_removed.lower()\n",
    "    \n",
    "#    handle negation patterns\n",
    "    negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \n",
    "                     \"weren't\":\"were not\",\"haven't\":\"have not\",\"hasn't\":\"has not\",\n",
    "                     \"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\", \n",
    "                     \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                     \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\n",
    "                     \"mightn't\":\"might not\", \"mustn't\":\"must not\"}\n",
    "    neg_pat = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "    neg_handled = neg_pat.sub(lambda x: negations_dic[x.group()], lower_text)\n",
    "    \n",
    "#    remove non-letter characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", neg_handled)\n",
    "    \n",
    "#    token and lemmatize words\n",
    "    words = [lemmatize(x) for x in tok.tokenize(letters_only)]\n",
    "#    filter out tokens with less than 1 characters\n",
    "    words = [x for x in words if len(x) > 1]\n",
    "#    filter out tokens of stopping words and meaningless words\n",
    "    meaningless_words = []\n",
    "#    stop_words = set(stopwords.words('english'))\n",
    "    words = [x for x in words if not x in meaningless_words]\n",
    "    \n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6e5978a8ce83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tweets['text'] = tweets['text'].apply(tweet_clean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pylab/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3847\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3848\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3849\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-9a77c99a95ed>\u001b[0m in \u001b[0;36mtweet_clean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mwww_pat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'www.[^ ]+'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcombined_pat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_pat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_pat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_pat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwww_pat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpat_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_pat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbom_removed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mlower_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpat_removed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pylab/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# tweets['text'] = tweets['text'].apply(tweet_clean)\n",
    "tweets['text'].apply(tweet_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_topics(tweets, meaningless_words, top_k):\n",
    "\n",
    "    tok = WordPunctTokenizer()\n",
    "    texts = tweets.text\n",
    "    meaningless_words = meaningless_words + ['hurricane', 'harvey', 'houstonstrong', 'due',\n",
    "                                             'hurricaneharvey', 'amp', 'houston', 'traffic',\n",
    "                                             'texas', 'tornado', 'storm', 'cdt', 'texasstrong']\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    meaningfull_list = []\n",
    "    for text in texts:\n",
    "        text_words = [x for x in tok.tokenize(text) if not x in stop_words + meaningless_words]\n",
    "        # for tweet with at least one meaningfull word\n",
    "        if len(text_words) > 0:\n",
    "            if len(text_words) == 1:\n",
    "                # remove word with less than 2 characters\n",
    "                if len(text_words[0]) > 2:\n",
    "                    meaningfull_list.append(text_words[0])\n",
    "            else:\n",
    "                text_words = [x for x in text_words if len(x) > 2]\n",
    "                meaningfull_list.append((\" \".join(text_words)).strip())\n",
    "    \n",
    "    if len(meaningfull_list) >= 1:\n",
    "        cvec = CountVectorizer()\n",
    "        X = cvec.fit_transform(meaningfull_list)\n",
    "        tn = pd.Series(cvec.get_feature_names())\n",
    "        tf = np.sum(X, axis=0)\n",
    "        tf = np.squeeze(np.asarray(tf))\n",
    "        idx = np.argsort(-tf)\n",
    "        if len(tn) > top_k:\n",
    "            top_words = pd.Series.tolist(tn[idx[:top_k]])\n",
    "        else:\n",
    "            top_words = pd.Series.tolist(tn[idx])\n",
    "        top_words = [top_words[ii] + '_' + str(tf[idx[ii]]) for ii in range(len(top_words))]\n",
    "    else:\n",
    "        top_words = np.nan\n",
    "    \n",
    "    return top_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
